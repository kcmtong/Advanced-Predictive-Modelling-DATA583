---
title: "Data 583 Life Expectancy (WHO)"
author: "Justin Chan, Kenny Tong, Viji Rajagopalan"
date: "22 Mar, 2023"
output:
  pdf_document: default
  html_document:
    df_print: paged
fontsize: 10pt
geometry: margin=2cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data 583 Life Expectancy - Final Report (Life Expectancy Data)

## Introduction and Hypotheses
Life Expectancy has always been an area of interest for humanity.  The dataset contains the Life Expectancy records of 193 countries between 2000-2015, together with different predictive factors. Broadly speaking, predicting variables are categorized into 4 major areas : Immmunization, Mortality, Economical, and Social, containing a total of 21 individual variables.

The primary purpose of this report is to compare and evaluate different predictive models in order to identify the most appropriate model for the dataset.  In particular, we will evaluate the applicability of the core assumptions of the selected models by methods such as Normality Test, Multicollinearity Assessment, etc. This could validate or decline the adoption of certain models because the model assumptions are simply not satisfied.  We are also going to verify whether the 4 predicting areas have equal significance on Life Expectancy, and whether there are adequate support evidence suggesting a strong correlation with the response variables.  Finally, we will also perform and compare fitting result of selected models, in particular whether parametric models would be more suitable than non-parametric models for this dataset. 

## Dataset overview

### Varaibles Types
|Variable     |Unit of Measurement/Data Category    | Continuous vs Discrete |
|-----        |-----                                | ----- |
|Life Expectancy |Years Old (Age)                   |Continuous|
|Country      |Nominal Data                         |Discrete|
|Year         |Ordinal Data                         |Discrete|
|Status       |Nominal Data                         |Discrete|
|Adult Mortality|Count Data                         |Discrete|
|Infant deaths|Count Data                           |Discrete|
|Under-five deaths|Count Data                       |Discrete|
|Hepatitis B  |Percentage                           |Continuous|
|Measles      |Count Data                           |Discrete|
|Polio        |Percentage                           |Continuous|
|Diphtheria   |Percentage                           |Continuous|
|Total expenditure|Percentage                       |Continuous|
|Percentage expenditure|Percentage                  |Continuous|
|GDP          |Currency (USD)                       |Continuous|
|Population   |Count                                |Discrete|
|Income composition of resources|Percentage         |Continuous|
|Schooling    |Mean (Years)                         |Continuous|
|Alcohol      |Litres                               |Continuous|
|HIV/AIDS     |Percentage                           |Continuous|
|BMI          |Average BMI                          |Continuous|
|Thinness 1-19 years|Percentage                     |Continuous|
|Thinness 5-9 years|Percentage                      |Continuous|

### Variables Summary and Categories
Life Expectancy is the response variable in this dataset.  This represents the mean of the life expectancy (in age) in a specific country in a given year.  For the data types of the predicting variables, most are percentage and count data across four major areas.  The first area is Immunization Data such as Hepatitis B and Polio (immunization coverage).  The second area is Mortality Data such as Adult Mortality/infant deaths (No. of deaths of Adult/infant per 1000 persons).  The third area is Economical Data such as GDP/Income composition/Percentage expenditure.  The fourth area is Social Data such as Schooling and Population.
 
To clean up and wrangle data for the subsequent analysis, NA data has been assessed.  A total of 2563 NA values are found in the dataset, spreading across a few columns.  These NA values are generally imputed by the respective column mean.   

### Checking for Multicollinearity
As Multicollinearity can potentially affect the accuracy of regression model and we have 22 variables, a correlation study is undertaken to understand and assess the situation.  A correlation plot has identified a number of correlation problems.  It is found that infant deaths and under.five.deaths are nearly 100% correlated.  The relation between the deaths rates of the two close age groups is easily interpretable.  In addition, there are three heavily correlated pairs which is defined by the abs(correlation coefficient)>0.7 between the variables.  They include (a) (immunization rate of) ‘Polio’-vs-‘Diphtheria’,  (b) ‘income composition of resources’-vs-‘Schooling’, and (c) between the two thinness measures for the age groups 5-9 vs 10-19.   Pairs (a) and (c) are justifiable while the relation for (b) demonstrate a relatively subtle relation.  Other than that, the degree of multicollinearity is acceptable and not too worrying.



```{r include=FALSE}
le <- read.csv("dataset/LifeExpectancy.csv")

# Create a new column Status.val to represent the Status column with number
le$Status.val <- ifelse(le$Status == "Developed",1,0)
# Create a new column as the scaled version of the GDP & Population, 
#le$GDP_scaled = scale(le$GDP)
#le$Population_scaled = scale(le$Population)
# Remove the unreliable column 
le <- subset(le,select=-c(percentage.expenditure))
```

```{r include=FALSE}
# Null Data Handling
library(magrittr) 
library(dplyr)  
library(tidyr)
le %>% group_by(Country) %>% summarise(COUNT = n())
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Null Data Analysis
library(magrittr) 
library(dplyr)  
library(tidyr)
missing.values <- le %>%
    gather(key = "key", value = "val") %>%
    mutate(is.missing = is.na(val)) %>%
    group_by(key, is.missing) %>%
    summarise(num.missing = n()) %>%
    filter(is.missing==T) %>%
    select(-is.missing) %>%
    arrange(desc(num.missing))

```


```{r message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(gridExtra)
missing.values <- le %>%
  gather(key="key", value="val") %>%
  mutate(isna=is.na(val)) %>%
  group_by(key) %>%
  mutate(total=n()) %>%
  group_by(key,total,isna) %>%
  summarise(num.isna=n()) %>%
  mutate(pct=num.isna/total * 100)
levels <- (missing.values%>%filter(isna==T) %>% arrange(desc(pct)))$key
null_percentage.plot <- missing.values %>% ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), stat='identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('dodgerblue2', 'coral'), 
                        labels = c("Present", "Missing")) +
      coord_flip() + labs(title = "Percentage of missing values", 
                          x = 'Features', y = "% of missing values")
null_inrow.plot <- le %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('dodgerblue2', 'coral'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Features", y = "Row Number", title = "Missing values in rows") +
    coord_flip()
library(dplyr)
#le_dropped <- le %>% filter_at(vars(Population_scaled,Population,GDP,GDP_scaled,Income.composition.of.resources,Schooling),any_vars(!is.na(.)))
le_dropped <- le %>% filter_at(vars(Population,GDP,Income.composition.of.resources,Schooling),any_vars(!is.na(.)))
missing.values <- le_dropped %>%
  gather(key="key", value="val") %>%
  mutate(isna=is.na(val)) %>%
  group_by(key) %>%
  mutate(total=n()) %>%
  group_by(key,total,isna) %>%
  summarise(num.isna=n()) %>%
  mutate(pct=num.isna/total * 100)
#missing.values
levels <- (missing.values%>%filter(isna==T) %>% arrange(desc(pct)))$key
null_percentage_dropped.plot <- missing.values %>% ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), stat='identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('dodgerblue2', 'coral'), 
                        labels = c("Present", "Missing")) +
      coord_flip() + labs(title = "Percentage of missing values after dropping some common null value records", 
                          x = 'Features', y = "% of missing values")
null_inrow_dropped.plot <- le_dropped %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('dodgerblue2', 'coral'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Features", y = "Row Number", title = "Missing values in rows after dropping some common null value records") +
    coord_flip()

options(repr.plot.width = 30, repr.plot.height = 30)
gridExtra::grid.arrange(null_percentage.plot, null_inrow.plot, ncol = 1)
gridExtra::grid.arrange(null_percentage_dropped.plot, null_inrow_dropped.plot, ncol = 1)

```


<!-- Check how much records do each country have:  -->
```{r message=FALSE, warning=FALSE, include=FALSE}
le %>% group_by(Country) %>% summarise(COUNT = n())
le_dropped %>% group_by(Country) %>% summarise(COUNT = n()) #12 country were removed after dropping some common null value (193-181)
#might need to consider not using this variable 
```


```{r message=FALSE, warning=FALSE, include=FALSE}
for(i in 1:ncol(le_dropped)) {                                   # Replace NA in all columns
  le_dropped[ , i][is.na(le_dropped[ , i])] <- mean(le_dropped[ , i], na.rm = TRUE)
}
```

## Statistical Analysis

#### Purpose
Do some visualization and initial statistical model assessments to explore and identify the general data pattern, trends and clusters, etc.

#### Procedure
### General Life Expectancy

As we are interested in Life Expectancy as our response variable, we first start looking at the distribution of the variable and general trend.

```{r echo=FALSE, fig.height=2, fig.width=2, message=FALSE, warning=FALSE}
library(ggplot2)
#install.packages("tidyverse")
library(tidyverse)
par(mfrow=c(1,2))
le_dropped %>%
  group_by(Year) %>%
  summarise(Life.expectancy = mean(Life.expectancy)) %>%
  ggplot(aes(x=Year,
             y=Life.expectancy)) +    
  geom_line()

le_dropped %>%
  group_by(Status) %>%
  summarise(Life.expectancy = mean(Life.expectancy)) %>%
  ggplot(aes(x=Status,
             y=Life.expectancy,
             fill=Status)) +    
  geom_bar(stat = "identity")+ scale_fill_manual(values=c('dodgerblue2', 'coral'))

```

#### Conclusion/Key Findings :
- The general life expectancy has been steadily increasing duration the year
- Average Life expectancy increase from about 67 to 71.5 in 15 years
- Life expectancy of Developed countries are significantly higher than that of Developing countries.

```{r echo=FALSE, fig.height=2, message=FALSE, warning=FALSE}
le_dropped.pivot <- pivot_longer(le_dropped,c(Adult.Mortality,under.five.deaths,infant.deaths),names_to='Mortality.Group',values_to='Mortality.Rate')
require(gridExtra)

le_dropped.pivot.area <- le_dropped.pivot %>%
  group_by(Year,Mortality.Group) %>%
  summarise(Mortality.Rate = mean(Mortality.Rate)) %>%
  ggplot(aes(x=Year,
             y=Mortality.Rate,
             fill=Mortality.Group)) +
  geom_area(position="stack",stat="identity")

le_dropped.pivot.line <- le_dropped.pivot %>%
  group_by(Year,Mortality.Group) %>%
  summarise(Mortality.Rate = mean(Mortality.Rate)) %>%
  ggplot(aes(x=Year,
             y=Mortality.Rate,
             color=Mortality.Group)) +
  geom_line()

grid.arrange(le_dropped.pivot.area,le_dropped.pivot.line, ncol=2)
```

Findings :

- The mortality rate of all three age groups are generally decreasing as a whole
- The mortality rate of the adult group, however, have fluctuation within the period


```{r message=FALSE, warning=FALSE, include=FALSE}
head(le_dropped)
```

```{r include=FALSE, warning=FALSE, include=FALSE}

#df = subset(le_dropped, select = -c(Country,GDP,Population) )
df = subset(le_dropped, select = -c(Country) )
#head(df)
```


For rest of the analysis, we drop the country column so that we can understand general trends in life expectancy
independent of countries. Let's visualize the distribution of response variable using below histogram.
```{r message=FALSE, warning=FALSE,echo=FALSE,fig.height=3,fig.width=3}
#library(Hmisc)
#library(GGally)
#ggpairs(df)
#df_hist <- df
#colnames(df_hist)[3]<-"LifeExp"
#hist.data.frame(df_hist[3])
```

Findings :
- As we see, the response variable Life Expectancy is normally distributed and so our first try is to see if MLR is able to predict well. 
- Also, using this model and running a BIC on it, we can understand the columns that are important.

## Predictor Space
We now turn our focus to look at the different predictor variables. Following shows the correlation of different variables and their spread in the dataset.

```{r message=FALSE, warning=FALSE, echo=FALSE}

#install.packages(GGally)
library(GGally)
ggcorr(df,palette = "RdBu", size=2,label=TRUE,label_size = 2,hjust = .95,layout.exp=2)

```

Summary of correlation for >0.7 or <-0.7: infant.deaths and under.five.deaths is nearly 100% correlated with each other and Polio is highly correlated with Diphtheria, Income composition of resources is highly correlated with Schooling.Both thinness variables are highly correlated. Other variables are low to moderately correlated.
The response variable Life expectancy is highly correlated with Income composition, Schooling and adult mortality variables.

### Initial Modelling and Variable Importance:
As response variable Life.Expectancy is approximately normally distributed, first step is to try lm model for this data and also run BIC to get the variable selection from the dataset.

The dataset has 20+ predictors and based on correlation plot there are correlation between the variables, BIC would help to eliminate some of the predictors that are conveying same signal as others and also explains less variability in life expectancy.

#### Full model summary and diagnostics
```{r message=FALSE, warning=FALSE,echo=FALSE}
lmmod <- lm(Life.expectancy~., data = df)
summary(lmmod)
```

```{r echo=FALSE,message=FALSE, warning=FALSE}
par(mfrow=c(1,2),heights=c(1,1,1,1))
plot(lmmod)
```

Residuals related plots: The residuals versus fits plot would provide us with information on the residual against the fitted values in regression analysis. This could be used to identify the patterns in the residuals that may indicate the model is not capturing the relationship between our predictor and the outcome variable, therefore, allowing us to detect any non-linearity, unequal error variances and outliers.  In general, we would want to see our residual randomly scattered around 0 since this indicates that the model assumption is met and is a good fit for the data. However, from the above-plotted residuals versus fits plot, we could see there is a curvature shape to our residuals and there is a presence of outliers and high leverage points on the left-hand side of the residuals versus fits the plot, This could be problematic since outliers and leverage points could have a significant impact on the regression coefficient. And the curved shape indicates that our model may be misspecified and further investigation is needed. Also, same is indicated by the scale-location plat with standardized residuals > 1.5. Based on leverage plot, we do not see the need to remove any outliers in this initial assessment. 

QQ plot: QQ-plot (Quantile-quantile plot) allow us to investigate the univariate normality of the dataset. If the points on the QQ-plot fall approximately along a straight line, it suggests that the sample comes from a population with similar distribution to the theoretical distribution that we are comparing to. From the QQ-plot that we have plotted above, the point deviates from a straight line on both ends and indicates there is a heavy tail. 

We make a note of the structures in the initial assessment and acknowledge they could impact the MLR performance. During next stage of the project, we plan to address the issues identified in diagnostics during final modeling.

#### Variable selection methods
Following is the summary from a couple of common methods used in variable section of models: BIC(Bayesian information criterion, backward selection) and VIF (Variable Inflation Factor).

```{r message=FALSE, warning=FALSE, include=FALSE}
model.step.bic <- step(lmmod,k=log(nrow(df)))
summary(model.step.bic)
```
Initially, all 20 variables were used in our model and achieved an AIC score of 7642.14. After performing BIC backward step model selection method, The BIC backward step model selection method has reduced our model’s independent variable to 13 and achieved a lower AIC score of 7604.34. Since a lower AIC score signifies the regression is a better fit to the data, meaning that after removing some irrelevant variable in our data set, the simple model is still able to explain the data well well and has improved the fitting from the initial model. Also the final reduced model has an Adjusted R-squared score of 0.8296 compared to the original model’s 0.8299 isn’t much of a drop in the Adjusted R-squared score meaning our BIC reduced model was still able to have the same amount of variability.

Now going back to variable selection, this time we will be using VIF(Variance Inflation Factor) to investigate whether it is possible to come up with a better model from BIC reduced model by eliminating some highly correlated variables in the data. 

```{r message=FALSE, warning=FALSE, include=FALSE}
library(car)
vif(model.step.bic)

#removed one of infant.deaths or under.five.deaths
```
VIF(Variance Inflation Factor) is a variable selection method that is used to identify and eliminate highly correlated variables in a regression model. If the VIF value for a variable is high, it indicates that the variable is highly correlated with another predictor within the model. From the above output, we could see that variables “infant.deaths” and “under.five.deaths” are highly correlated. Therefore we will have to remove either “infant.deaths” or “under.five.deaths” to resolve the multicollinearity within our data inorder to improve our model accuracy and interpretability. We acknowledge this may be necessary for next level of tuning the model.
```{r message=FALSE, warning=FALSE, include=FALSE}
df1<-df[,c('Life.expectancy','Status','Adult.Mortality','infant.deaths','under.five.deaths',
      'Hepatitis.B','BMI','Polio','Diphtheria',
      'HIV.AIDS','thinness..1.19.years','Income.composition.of.resources','Schooling','GDP')]

df1$Status <- factor(df1$Status)
#removed 'under.five.deaths', due to vif - multi collinearity from BIC selection but it decreased model
#performance so should we retain it?
#head(df1)
```

After doing variable reduction and selecting only variables that are recommended by BIC (Status + Adult.Mortality + infant.deaths + Hepatitis.B + BMI + under.five.deaths + Polio + Diphtheria + HIV.AIDS + GDP + thinness..1.19.years + Income.composition.of.resources + Schooling), we move on checking for any clustering effects in data. 

```{r message=FALSE, warning=FALSE}
library(mclust)
clus1 <- Mclust(df1)
summary(clus1)
```
#### Clustering
Looks like there are some clusters in the data, first understanding is it is because of variable “Status”’s developed vs developing. Acknowledging this information which may be helpful in future phases of model building and fine tuning. For example, if MLR would be the final model, building interaction with cluster variable and rest of data would further improve model performance.

Using our reduced model, we feed that data to Linear Model and achieve an Adjusted R-squared of 0.8296 and all variables have a p-value of less the 0.05, meaning that all of our independent variables within the model is statically significant to our dependent variable, In other words, there is strong evidence against the null hypothesis, suggesting that the observed relation between other dependent variable and independent variable is significant and real, not just due to random variation or chance.

```{r message=FALSE, warning=FALSE, include=FALSE}
lmmod2 <- lm(Life.expectancy~.,data=df1)
summary(lmmod2)
vif(lmmod2)
```
### Conclusion/Key Findings
Summarized below are some key findings from EDA.

- Response variable is looking to be normally distributed and initial model score is ~82% which means this model is able to explain 82% of variation in Life expectancy. Its possible to use multiple linear regression for this data. From the diagnostic plots it may be seen that there is skewness in the data.
- There are some variables that suffer multi collinearity (from VIF) scores.
- Not all predictors are necessary to describe response variable. Model selection will be helpful.

Due to the spread of data (clustering, non-linearity of predictors w response variable, skewness in data), it is necessary to explore other models specifically non-parametric regression models.


## Questions and Next Steps
1. Does the variables selected using BIC and linear model able to explain Life Expectancy adequately? A hypotheses test is required for this.

2. Is the response variable normally distributed? Shapiro-Wilk test for normality will need to be conducted for this.

3. Is Multi linear regression the best model or go with other non parametric models? From initial feedback, there are hypotheses tests available to validate this.

Need to explore further on these questions from proposal stage and conclude.

4. Understanding impact of individually controlled factors - The dataset has all predicting variables divided into 4 groups: Immunization related factors, Mortality factors, Economical factors and Social factors. Some of these factors are controllable by individuals like immunization, alcohol etc. Some of these factors are noncontrollable and macro elements like GDP. If an individual within a country want to improve life expectancy, how much is controllable/can be influenced personally? What proportion of variation in life expectancy can be explained by these variables? For example, What is the effect of “Alcohol/BMI” on the life expectancy?

5. Understanding impact of Government/Public controlled factors - From Government perspective, how are the preventive measures influencing life expectancy? What proportion of variation in life expectancy can be explained by these variables? For example, Does Higher health expenditure (column H) on Health improve life expectancy?



```{r message=FALSE, warning=FALSE, include=FALSE}
## Extra - add if there is page available

#str(le)
class(df$Country)
glm_model <- glm(Life.expectancy~., data = df, family = "gaussian")
bic_back <- step(glm_model, k=log(nrow(df)), direction="backward", trace=FALSE)
summary(bic_back)
summary(glm_model)
```



```{r message=FALSE, warning=FALSE, include=FALSE}
###############################################################################
###############PCA(suggest a simpler scoring system):##########################
###############################################################################
#head(df[,-3])
head(df[,c(-1,-3)])
df$Status <- as.numeric(as.factor(df$Status)) #Convert Non-Numeric Columns to Numeric for PCA
pca_model <- prcomp(df[,c(-1,-3)],scale.=TRUE)
summary(pca_model)

```
```{r message=FALSE, warning=FALSE, include=FALSE}
#Use Sceen Plot to decide how many PC to keep, But there seems to be a large value in our PC1 Standard Deviation so it isn't a clear indicator.

plot(pca_model, type="lines")
```
```{r message=FALSE, warning=FALSE, include=FALSE}
#Use Standard deviation instead to decide how many PC to keep, if it is below 1 we toss it out since it does not explain much info about our data.it is suggest us to keep 15 PC.

pca_model$rotation[,1:15]

###############################################################################
#########################################FA:(not working)###################################
###############################################################################


df2 = subset(le_dropped, select = -c(Country) )
head(df2)

#Need to eliminate non numeric variables before running factor analysis below
head(df)
#fa_model <- factanal(df2, factors = 13)
```



1. normal distribution test for our y variable 

```{r}
#Histogram & QQPlot
par(mfrow=c(1,2)) 
hist(df$Life.expectancy, col='steelblue', main='Life.expectancy_Histogram')
#not really a good "bell-shape"
qqnorm(df$Life.expectancy, main='Life.expectancy_QQplot')
#most of the data is not fall along a straight diagonal line 
qqline(df$Life.expectancy)

#Both are indicating that our predict variable Y "df$Life.expectancy" is not normally distributed 
```

```{r}
#Shapiro-Wilk Test

shapiro.test(df$Life.expectancy)

#Finding: Since df$Life.expectancy p-value is less than .05, indicate that our y variable is not normally distributed!!!
```


```{r}
#Kolmogorov-Smirnov Test 

ks.test(df$Life.expectancy, 'pnorm')

#Finding: Since df$Life.expectancy p-value is less than .05, indicate that our y variable is not normally distributed!!!
```
########################################
LM with matching dependent variable with npreg
```{r}

model_lm <- lm(Life.expectancy~Adult.Mortality + infant.deaths + Hepatitis.B + BMI + under.five.deaths + Polio + Diphtheria + HIV.AIDS + GDP + thinness..1.19.years + Income.composition.of.resources + Schooling, data = df, x=TRUE, y=TRUE)
summary(model_lm)
```
########################################

2. npreg on our dataset?  

Note: When have time need to rerun with "x=True, y=True", "VIF remove", "add status.val", "train split"
```{r cache=TRUE}
library(np)
# n <- names(df)
# f <- as.formula(paste("df$Life.expectancy ~", paste(n[!n %in% "Life.expectancy"], collapse = " + ")))
# 
# model_np <- npregbw(Life.expectancy ~ Adult.Mortality + infant.deaths + Hepatitis.B + BMI + under.five.deaths + Polio + Diphtheria + HIV.AIDS + GDP + thinness..1.19.years + Income.composition.of.resources + Schooling, data = df, regtype="ll", bwmethod = "cv.aic")    #19 HRs to run...

# model_np <- npreg(bws = model_np)
# summary(model_np)
model_np <- readRDS("model_np.rds") #PreTrained Model
summary(model_np)
```
```{r}
# objects()
# find("model_np")
# 
# saveRDS(model_np,"model_np.rds")

```

```{r cache=TRUE}
#npsigtest_npreg <- npsigtest(model_np)    #10 HRs to run...
```
![npsigtest_npreg result](dataset/npsigtest_npreg.png)

3. LASSO and Neuralnet
Two different supervised algorithms tried on the dataset. They do not have the constraint of a normal distribution for response variable.

First did a train and test split so we can measure the MSE and compare how each of the models are performing interms of minimizing MSE.

```{r}
library(glmnet)
#70:30 split for train and test

df1<-df[,c('Life.expectancy','Adult.Mortality','infant.deaths','under.five.deaths',
      'Hepatitis.B','BMI','Polio','Diphtheria',
      'HIV.AIDS','thinness..1.19.years','Income.composition.of.resources','Schooling','GDP','Status.val')]

ind <- sample(1:nrow(df1), 2000)
traino <- df1[ind,]
testo <- df1[-ind,]
```


Linear model
```{r}
lmmodtr <- lm(traino[,1]~.,data=traino[,-1],x=TRUE, y=TRUE)
summary(lmmodtr)

```
Another test to see if the above parametric model specification is correct.
```{r}
library(lmtest)
resettest(lmmodtr)
```
```{r}
#LASSO
library(glmnet)
y <- traino$Life.expectancy
x <- data.matrix(traino[,-1])
#k-fold cross-validation to find optimal lambda value\
#cv default is 10 fold
cv_model <- cv.glmnet(x, y, alpha = 1)

#optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
plot(cv_model) 

#coefficients of best model
lasmod <- glmnet(as.matrix(traino[,-1]),traino$Life.expectancy, alpha = 1, lambda = best_lambda)
coef(lasmod)
```
```{r}
#linear model
mselm_te1 <- mean((testo[,1]-predict(lmmodtr, newdata=testo))^2)

#lasso
mselas_te1 <- mean((testo[,1]-predict(lasmod, newx=as.matrix(testo[,-1])))^2)
print(mselm_te1)
print(mselas_te1)
#MSE comparison

```
```{r}
library(nnet)
#18 MSE
for(i in 1:62){
  
  set.seed(4521)
  
  train_lin <- nnet(traino[,1]~., data=traino, size=i, linout=TRUE, trace=FALSE)
#calculating mse

  mse_nnet_lin <- mean((testo[,1]-(predict(train_lin, newdata=testo)))^2)
  print(paste("Number of hidden layer variables:", i))
  print(paste("MSE:",mse_nnet_lin))
  
}
```

##Pending
1. np specification test
```{r cache=TRUE}
# X <- data.frame(df$Adult.Mortality,df$infant.deaths,df$Hepatitis.B,df$BMI,df$under.five.deaths,df$Polio,df$Diphtheria,df$HIV.AIDS,df$GDP,df$thinness..1.19.years,df$Income.composition.of.resources,df$Schooling)
# 
# result_npcms <- npcmstest(model=model_lm, xdat=X, ydat=df$Life.expectancy) #33Hours to run
```
```{r}
# result_npcms

# objects()
# find("result_npcms")
# 
# saveRDS(result_npcms,"result_npcms.rds")
result_npcms <- readRDS("result_npcms.rds") #PreTrained Model
summary(result_npcms)
```
2. Visualizing bimodal distribution of yS
```{r}
#Histogram & QQPlot
par(mfrow=c(1,2)) 
hist(df$Life.expectancy, col='steelblue', main='Life.expectancy_Histogram',breaks = 35)
#not really a good "bell-shape"
qqnorm(df$Life.expectancy, main='Life.expectancy_QQplot')
S#most of the data is not fall along a straight diagonal line 
qqline(df$Life.expectancy)

#Both are indicating that our predict variable Y "df$Life.expectancy" is not normally distributed 
```