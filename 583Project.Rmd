---
title: "Data 583 Life Expectancy (WHO)"
author: "Justin Chan, Kenny Tong, Viji Rajagopalan"
date: "7 Mar, 2023"
output: pdf_document
fontsize: 10pt
geometry: margin=2cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## EDA


### Original Dataset Summary & Initial Data Screening
Purpose : Let's take a snapshot of the original dataset and have a rough idea of its record
```{r}
le <- read.csv("dataset/LifeExpectancy.csv")
summary(le)
```
Let's look at the dataset dimension first
```{r}
dim(le)
```

Then, have a quick overall screening of the dataset
```{r}
#NOTE: might consider to remove this since str(le) provided us same information but in a more presentable 
#head(le,5)

```

Here is another view :
```{r}
str(le)
```

From the above broad view, the following Conclusion/Key Findings are reached : 

- The records range is from Year 2000 to 2015
- Columns with NA : Life Expectancy, Adult Mortality, Alcohol, Hep B, BMI, Polio, Total exp, Dip, GDP, Population, thinness..1.19, thinness.5.9, Income.composition.of.resources, Schooling
- 'Status' Column is of the "character" data type, with values "Developing" and "Developed".  We will introduce a new column 'Status.val' to be the factor value of 'Status' for better analysis..
- 'Percentage Expenditure' has a mean value of `r mean(le$percentage.expenditure)` and max. value of `r max(le$percentage.expenditure)`.  Spending on health is more than the GDP per capita? Look into the column definition : Expenditure on health as a percentage of Gross Domestic Product per capita(%). The data of such magnitude simply does not quite make sense. Cross check with other references (e.g. the World Bank https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS).  OK, let's conclude that we have hesitation about the reliability/interpretation of the value of this column, and probably would drop and skip this column for the rest of this analysis.
- 'Population' and 'GDP' have a relatively large scale, compared with all other columns.  So, we may need to scale these two columns.

Now, let's do some data wrangling based on the above conclusions :
```{r}
# Create a new column Status.val to represent the Status column with number
le$Status.val <- ifelse(le$Status == "Developed",1,0)

# Create a new column as the scaled version of the GDP & Population, 
le$GDP_scaled = scale(le$GDP)
le$Population_scaled = scale(le$Population)

# Remove the unreliable column 
le <- subset(le,select=-c(percentage.expenditure))

```

### Null Value Analysis and Handling
```{r}
library(magrittr) 
library(dplyr)  
library(tidyr)
le %>% group_by(Country) %>% summarise(COUNT = n())
```
Purpose : Investigate the and determine how to handle the null value in the data set

Missing values could have a large affect to the overall quality of the static models and machine learning models and need to be clean before using it in our training model. 

Lets investigate how many missing values within our features:

```{r warning = FALSE, message=FALSE}
library(magrittr) 
library(dplyr)  
library(tidyr)

missing.values <- le %>%
    gather(key = "key", value = "val") %>%
    mutate(is.missing = is.na(val)) %>%
    group_by(key, is.missing) %>%
    summarise(num.missing = n()) %>%
    filter(is.missing==T) %>%
    select(-is.missing) %>%
    arrange(desc(num.missing)) 

missing.values
```
There are total of 2563 missing value within our dataset, we could visualize the missing data to identify patterns or cluster of missing values within our data to determine the cause of the missing data and whether it is random or systematic and to highlight potential biases that may exist in our data set. Visualizing the missing value also allow to understand the extend of the missing data and determine appropriate strategies for imputing missing value, since different imputation methods could be more appropriate depending on the pattern of the missing data.

```{r}
library(ggplot2)
library(gridExtra)

missing.values <- le %>%
  gather(key="key", value="val") %>%
  mutate(isna=is.na(val)) %>%
  group_by(key) %>%
  mutate(total=n()) %>%
  group_by(key,total,isna) %>%
  summarise(num.isna=n()) %>%
  mutate(pct=num.isna/total * 100)

levels <- (missing.values%>%filter(isna==T) %>% arrange(desc(pct)))$key

null_percentage.plot <- missing.values %>% ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), stat='identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('dodgerblue2', 'coral'), 
                        labels = c("Present", "Missing")) +
      coord_flip() + labs(title = "Percentage of missing values", 
                          x = 'Features', y = "% of missing values")

null_inrow.plot <- le %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('dodgerblue2', 'coral'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Features", y = "Row Number", title = "Missing values in rows") +
    coord_flip()

library(dplyr)
le_dropped <- le %>% filter_at(vars(Population_scaled,Population,GDP,GDP_scaled,Income.composition.of.resources,Schooling),any_vars(!is.na(.)))

missing.values <- le_dropped %>%
  gather(key="key", value="val") %>%
  mutate(isna=is.na(val)) %>%
  group_by(key) %>%
  mutate(total=n()) %>%
  group_by(key,total,isna) %>%
  summarise(num.isna=n()) %>%
  mutate(pct=num.isna/total * 100)

missing.values

levels <- (missing.values%>%filter(isna==T) %>% arrange(desc(pct)))$key

null_percentage_dropped.plot <- missing.values %>% ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), stat='identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('dodgerblue2', 'coral'), 
                        labels = c("Present", "Missing")) +
      coord_flip() + labs(title = "Percentage of missing values after dropping some common null value records", 
                          x = 'Features', y = "% of missing values")

null_inrow_dropped.plot <- le_dropped %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('dodgerblue2', 'coral'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Features", y = "Row Number", title = "Missing values in rows after dropping some common null value records") +
    coord_flip()

options(repr.plot.width = 30, repr.plot.height = 60)

gridExtra::grid.arrange(null_percentage.plot, null_inrow.plot, ncol = 1)
gridExtra::grid.arrange(null_percentage_dropped.plot, null_inrow_dropped.plot, ncol = 1)


```

There are `r dim(le)[1]` no. of rows in the dataset. Acording to our Visualization on left, there seems to be a correlation in the appearance in missing data in our orginal data's feature "population", "gdp" , "income.composition.of.resources" and "schooling". We could deal with this correlation in missing data by removing the the record that have missing value in all of the listed variables.

#Check how much records do each country have:
```{r}
le %>% group_by(Country) %>% summarise(COUNT = n())
le_dropped %>% group_by(Country) %>% summarise(COUNT = n()) #12 country were removed after dropping some common null value (193-181)

#might need to consider not using this variable 
```

########Since we have a better approach, we might need to remove this############
Let's set the threshold of 20% as the max. proportion of null column to be allowed in a data column.  That means, columns with na over 20% will be dropped.  The threshold is then  `r dim(le)[1] * 0.2`.  So, the following 'Population' column will be dropped.
____________________________________________________

#head(le)
le <- subset(le,select=-c(Population))
# also Population_Scaled
le <- subset(le,select=-c(Population_scaled))

____________________________________________________
#################################################################################

For the other values, we will set the na to the respective column mean for the subsequent analysis.

```{r message=FALSE, warning=FALSE}

for(i in 1:ncol(le_dropped)) {                                   # Replace NA in all columns
  le_dropped[ , i][is.na(le_dropped[ , i])] <- mean(le_dropped[ , i], na.rm = TRUE)
}

```

Now, all na have been handled!  Let's continue our analysis.




### Overall General Life Expectancy Trend

Purpose : Do some visualisation to explore and identify the general data pattern, trends and clusters, etc

```{r fig.height=3, fig.width=3, message=FALSE, warning=FALSE}
library(ggplot2)
#install.packages("tidyverse")
library(tidyverse)

le_dropped %>%
  group_by(Year) %>%
  summarise(Life.expectancy = mean(Life.expectancy)) %>%
  ggplot(aes(x=Year,
             y=Life.expectancy)) +    
  geom_line()

```

Findings :

- The general life expectancy has been steadily increasing duration the year
- Average Life expectancy increase from about 67 to 71.5 in 15 years.



```{r fig.height=3, fig.width=3, message=FALSE, warning=FALSE}
le_dropped %>%
  group_by(Status) %>%
  summarise(Life.expectancy = mean(Life.expectancy)) %>%
  ggplot(aes(x=Status,
             y=Life.expectancy,
             fill=Status)) +    
  geom_bar(stat = "identity")+ scale_fill_manual(values=c('dodgerblue2', 'coral'))

```
Finding :

- Life expectancy of Developed countries are significantly higher than that of Developing countries.

```{r fig.height=3, message=FALSE, warning=FALSE}
le_dropped.pivot <- pivot_longer(le_dropped,c(Adult.Mortality,under.five.deaths,infant.deaths),names_to='Mortality.Group',values_to='Mortality.Rate')
require(gridExtra)

le_dropped.pivot.area <- le_dropped.pivot %>%
  group_by(Year,Mortality.Group) %>%
  summarise(Mortality.Rate = mean(Mortality.Rate)) %>%
  ggplot(aes(x=Year,
             y=Mortality.Rate,
             fill=Mortality.Group)) +
  geom_area(position="stack",stat="identity")

le_dropped.pivot.line <- le_dropped.pivot %>%
  group_by(Year,Mortality.Group) %>%
  summarise(Mortality.Rate = mean(Mortality.Rate)) %>%
  ggplot(aes(x=Year,
             y=Mortality.Rate,
             color=Mortality.Group)) +
  geom_line()

grid.arrange(le_dropped.pivot.area,le_dropped.pivot.line, ncol=2)
```

Findings :

- The mortality rate of all three age groups are generally decreasing as a whole
- The mortality rate of the adult group, however, have fluctuation within the period




###############################################################################
######################Variable Selections (BIC backwards):#####################
###############################################################################




```{r}
head(le_dropped)
```
```{r}
#droped GDP,Population
df = subset(le_dropped, select = -c(Country,GDP,Population) )
#droped GDP_scaled,Population_scaled
df_ns = subset(le_dropped, select =-c(Country,GDP_scaled,Population_scaled) )
head(df)
```
Let's look at the distribution of different columns in the dataset.
```{r}
#install.packages("Hmisc")
#library(Hmisc)
#hist.data.frame(df)
```
As we see, the response variable Life Expectancy is normally distributed and so our first try is to see if 
MLR is able to predict well. Also, using this model and running a BIC on it, we can understand the columns that are
important.
```{r}
lmmod <- lm(Life.expectancy~., data = df)
summary(lmmod)
```
```{r}
plot(lmmod)
```


```{r}
model.step.bic <- step(lmmod,k=log(nrow(df)))
summary(model.step.bic)
```


try clustering to understand grouping in data so we can separate may be developing vs
developed countries

From below results, we do not see any such cluster to split. 

```{r}
library(mclust)
clus <- Mclust(df)
summary(clus)

#No clusters seen in original dataset
```
Variable selection using BIC and VIF, VIF also increases model performance:

```{r}
install.packages("car")
library(car)
vif(model.step.bic)

#removed one of infant.deaths or under.five.deaths
```
```{r}
df1<-df[,c('Life.expectancy','Status','Adult.Mortality','infant.deaths','under.five.deaths',
      'Hepatitis.B','BMI','Polio','Diphtheria',
      'HIV.AIDS','thinness..1.19.years','Income.composition.of.resources','Schooling','GDP_scaled')]

df1$Status <- factor(df1$Status)
#removed 'under.five.deaths', due to vif - multi collinearity from BIC selection but it decreased model
#performance so should we retain it?
head(df1)
```
```{r}
library(mclust)
clus1 <- Mclust(df1)
summary(clus1)

#looks like it is finding some clusters, may be developed vs developing? If so, we need to add that info 
#and build interaction
```


```{r}
lmmod2 <- lm(Life.expectancy~.,data=df1)
summary(lmmod2)
vif(lmmod2)
```

```{r}
#rcorr(df1)
head(df1)
library(ggplot2)
ggplot(df1, aes(Adult.Mortality, infant.deaths)) + 
  geom_tile()
```

```{r}
#str(le)
class(df$Country)
glm_model <- glm(Life.expectancy~., data = df, family = "gaussian")
bic_back <- step(glm_model, k=log(nrow(df)), direction="backward", trace=FALSE)
summary(bic_back)
summary(glm_model)
```


```{r}
library("glmnet")
#head(df_ns) #using non-scaled GDP and Population
df_ns$Status <- as.numeric(as.factor(df_ns$Status)) #Convert Non-Numeric Columns to Numeric for glmnet
y <- df_ns$Life.expectancy
x <- as.matrix(df_ns[,c(-1,-3)])

grid <- exp(seq(2, -9, length=100)) 

ladf<- cv.glmnet(x, y, alpha=1, lambda=grid, standardize = TRUE)
plot(ladf$glmnet.fit, label=TRUE, xvar="lambda")
plot(ladf)

coef(ladf) #thinness.5.9.years is removed since the coefficients went to 0
````
```{r}
library("glmnet")
df$Status <- as.numeric(as.factor(df$Status)) #Convert Non-Numeric Columns to Numeric for glmnet
y <- df$Life.expectancy
x <- as.matrix(df[,c(-1,-3)])
grid <- exp(seq(2, -9, length=100)) 

ladf<- cv.glmnet(x, y, alpha=1, lambda=grid, standardize = TRUE)

plot(ladf$glmnet.fit, label=TRUE, xvar="lambda")
plot(ladf)

coef(ladf)
````



###############################################################################
###############PCA(suggest a simpler scoring system):##########################
###############################################################################

```{r}

#head(df[,-3])
head(df[,c(-1,-3)])
df$Status <- as.numeric(as.factor(df$Status)) #Convert Non-Numeric Columns to Numeric for PCA
pca_model <- prcomp(df[,c(-1,-3)],scale.=TRUE)
summary(pca_model)

```
```{r}
#Use Sceen Plot to decide how many PC to keep, But there seems to be a large value in our PC1 Standard Deviation so it isn't a clear indicator.

plot(pca_model, type="lines")
```
```{r}
#Use Standard deviation instead to decide how many PC to keep, if it is below 1 we toss it out since it does not explain much info about our data.it is suggest us to keep 15 PC.

pca_model$rotation[,1:15]
```
###############################################################################
#########################################FA:(not working)###################################
###############################################################################

```{r}
df2 = subset(le_dropped, select = -c(Country,GDP_scaled,Population_scaled) )
head(df2)

head(df)
fa_model <- factanal(df2, factors = 13)

```
