---
title: "Data 583 Life Expectancy (WHO)"
author: "Justin Chan, Kenny Tong, Viji Rajagopalan"
date: "7 Mar, 2023"
output:
  pdf_document: default
  html_document:
    df_print: paged
fontsize: 10pt
geometry: margin=2cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data 583 Life Expectancy - Exploratory Data Analysis (Life Expectancy Data)

## Introduction
The main goal of assessing the Life expectancy dataset is to understand the variables that impact life expectancy and also to know how well life expectancy is described by the selected variables. In this document, we take a detailed look at different aspects of the data like summary and apply statistical techniques to understand the underlying
structures. Also, based on the assessments, we will come up with next steps to fine tune data and improve modeling.

## Data Exploration

### Original Dataset Summary & Initial Data Screening
#### Purpose 
- Let's take a snapshot of the original dataset and have a rough idea of its record

#### Procedure
Take a look at the dataset summary.
```{r include=FALSE}
le <- read.csv("dataset/LifeExpectancy.csv")

```
```{r}
summary(le)
```

Here is another overview that gives information on type of variable, sample data from each column below.
This also gives number of records and columns which is 2938 and 22 respectively.
```{r echo=FALSE}
str(le)
```

#### Conclusion/Key Findings : 
- The records range is from Year 2000 to 2015
- Columns with NA : Life Expectancy, Adult Mortality, Alcohol, Hep B, BMI, Polio, Total exp, Dip, GDP, Population, thinness..1.19, thinness.5.9, Income.composition.of.resources, Schooling
- 'Status' Column is of the "character" data type, with values "Developing" and "Developed".  We will introduce a new column 'Status.val' to be the factor value of 'Status' for better analysis..
- 'Percentage Expenditure' has a mean value of `r mean(le$percentage.expenditure)`.  Spending on health is more than the GDP per capita (!?).  Look into the column definition : Expenditure on health as a percentage of Gross Domestic Product per capita(%). The data of such magnitude simply does not quite make sense. Cross check with other references (e.g. the World Bank https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS).  OK, let's conclude that we have hesitation about the reliability/interpretation of the value of this column, and this column would be dropped for the rest of this analysis.

```{r include=FALSE}
# Create a new column Status.val to represent the Status column with number
le$Status.val <- ifelse(le$Status == "Developed",1,0)
# Create a new column as the scaled version of the GDP & Population, 
#le$GDP_scaled = scale(le$GDP)
#le$Population_scaled = scale(le$Population)
# Remove the unreliable column 
le <- subset(le,select=-c(percentage.expenditure))
```
### Null Value Analysis and Handling
```{r include=FALSE}
library(magrittr) 
library(dplyr)  
library(tidyr)
le %>% group_by(Country) %>% summarise(COUNT = n())
```
#### Purpose 
- Investigate the and determine how to handle the null value in the data set.  Missing values could have a large affect to the overall quality of the static models and machine learning models and need to be clean before using it in our training model. 
#### Procedure
Lets investigate how many missing values within our features.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(magrittr) 
library(dplyr)  
library(tidyr)
missing.values <- le %>%
    gather(key = "key", value = "val") %>%
    mutate(is.missing = is.na(val)) %>%
    group_by(key, is.missing) %>%
    summarise(num.missing = n()) %>%
    filter(is.missing==T) %>%
    select(-is.missing) %>%
    arrange(desc(num.missing))

```

A brief check indicates that there are total of 2563 missing value within our dataset, we could visualize the missing data to identify patterns or cluster of missing values within our data to determine the cause of the missing data and whether it is random or systematic and to highlight potential biases that may exist in our data set. Visualizing the missing value also allow to understand the extend of the missing data and determine appropriate strategies for imputing missing value. Refer figure in next page, there are `r dim(le)[1]` no. of rows in the dataset. According to our Visualization, there seems to be a correlation in the appearance in missing data in our original data's feature "population", "gdp" , "income.composition.of.resources" and "schooling". We would deal with this correlation in missing data by removing the the record that have missing value in all of the listed variables.
For the other values, we would set the na to the respective column mean for the subsequent analysis.

#### Conclusion/Key Findings :

- na values have been analyzed 
- Data imputation have been performed as far as possible in order to prepare for the subsequent data analysis.

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(gridExtra)
missing.values <- le %>%
  gather(key="key", value="val") %>%
  mutate(isna=is.na(val)) %>%
  group_by(key) %>%
  mutate(total=n()) %>%
  group_by(key,total,isna) %>%
  summarise(num.isna=n()) %>%
  mutate(pct=num.isna/total * 100)
levels <- (missing.values%>%filter(isna==T) %>% arrange(desc(pct)))$key
null_percentage.plot <- missing.values %>% ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), stat='identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('dodgerblue2', 'coral'), 
                        labels = c("Present", "Missing")) +
      coord_flip() + labs(title = "Percentage of missing values", 
                          x = 'Features', y = "% of missing values")
null_inrow.plot <- le %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('dodgerblue2', 'coral'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Features", y = "Row Number", title = "Missing values in rows") +
    coord_flip()
library(dplyr)
#le_dropped <- le %>% filter_at(vars(Population_scaled,Population,GDP,GDP_scaled,Income.composition.of.resources,Schooling),any_vars(!is.na(.)))
le_dropped <- le %>% filter_at(vars(Population,GDP,Income.composition.of.resources,Schooling),any_vars(!is.na(.)))
missing.values <- le_dropped %>%
  gather(key="key", value="val") %>%
  mutate(isna=is.na(val)) %>%
  group_by(key) %>%
  mutate(total=n()) %>%
  group_by(key,total,isna) %>%
  summarise(num.isna=n()) %>%
  mutate(pct=num.isna/total * 100)
#missing.values
levels <- (missing.values%>%filter(isna==T) %>% arrange(desc(pct)))$key
null_percentage_dropped.plot <- missing.values %>% ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), stat='identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('dodgerblue2', 'coral'), 
                        labels = c("Present", "Missing")) +
      coord_flip() + labs(title = "Percentage of missing values after dropping some common null value records", 
                          x = 'Features', y = "% of missing values")
null_inrow_dropped.plot <- le_dropped %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('dodgerblue2', 'coral'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Features", y = "Row Number", title = "Missing values in rows after dropping some common null value records") +
    coord_flip()

options(repr.plot.width = 30, repr.plot.height = 30)
gridExtra::grid.arrange(null_percentage.plot, null_inrow.plot, ncol = 1)
gridExtra::grid.arrange(null_percentage_dropped.plot, null_inrow_dropped.plot, ncol = 1)

```


<!-- Check how much records do each country have:  -->
```{r message=FALSE, warning=FALSE, include=FALSE}
le %>% group_by(Country) %>% summarise(COUNT = n())
le_dropped %>% group_by(Country) %>% summarise(COUNT = n()) #12 country were removed after dropping some common null value (193-181)
#might need to consider not using this variable 
```

<!-- ########Since we have a better approach, we might need to remove this############ -->
<!-- Let's set the threshold of 20% as the max. proportion of null column to be allowed in a data column.  That means, columns with na over 20% will be dropped.  The threshold is then  `r dim(le)[1] * 0.2`.  So, the following 'Population' column will be dropped.  -->
<!-- ____________________________________________________ -->

<!-- #head(le) -->
<!-- le <- subset(le,select=-c(Population)) -->
<!-- # also Population_Scaled -->
<!-- #le <- subset(le,select=-c(Population_scaled)) -->

<!-- ____________________________________________________ -->
<!-- ################################################################################# -->

```{r message=FALSE, warning=FALSE, include=FALSE}
for(i in 1:ncol(le_dropped)) {                                   # Replace NA in all columns
  le_dropped[ , i][is.na(le_dropped[ , i])] <- mean(le_dropped[ , i], na.rm = TRUE)
}
```

## Statistical Analysis

#### Purpose
Do some visualization and initial statistical model assessments to explore and identify the general data pattern, trends and clusters, etc.

#### Procedure

### General Life Expectancy

As we are interested in Life Expectancy as our response variable, we first start looking at the distribution of the variable and general trend.

```{r echo=FALSE, fig.height=2, fig.width=2, message=FALSE, warning=FALSE}
library(ggplot2)
#install.packages("tidyverse")
library(tidyverse)
par(mfrow=c(1,2))
le_dropped %>%
  group_by(Year) %>%
  summarise(Life.expectancy = mean(Life.expectancy)) %>%
  ggplot(aes(x=Year,
             y=Life.expectancy)) +    
  geom_line()

le_dropped %>%
  group_by(Status) %>%
  summarise(Life.expectancy = mean(Life.expectancy)) %>%
  ggplot(aes(x=Status,
             y=Life.expectancy,
             fill=Status)) +    
  geom_bar(stat = "identity")+ scale_fill_manual(values=c('dodgerblue2', 'coral'))

```

#### Conclusion/Key Findings :

- The general life expectancy has been steadily increasing duration the year
- Average Life expectancy increase from about 67 to 71.5 in 15 years
- Life expectancy of Developed countries are significantly higher than that of Developing countries.

```{r echo=FALSE, fig.height=2, message=FALSE, warning=FALSE}
le_dropped.pivot <- pivot_longer(le_dropped,c(Adult.Mortality,under.five.deaths,infant.deaths),names_to='Mortality.Group',values_to='Mortality.Rate')
require(gridExtra)

le_dropped.pivot.area <- le_dropped.pivot %>%
  group_by(Year,Mortality.Group) %>%
  summarise(Mortality.Rate = mean(Mortality.Rate)) %>%
  ggplot(aes(x=Year,
             y=Mortality.Rate,
             fill=Mortality.Group)) +
  geom_area(position="stack",stat="identity")

le_dropped.pivot.line <- le_dropped.pivot %>%
  group_by(Year,Mortality.Group) %>%
  summarise(Mortality.Rate = mean(Mortality.Rate)) %>%
  ggplot(aes(x=Year,
             y=Mortality.Rate,
             color=Mortality.Group)) +
  geom_line()

grid.arrange(le_dropped.pivot.area,le_dropped.pivot.line, ncol=2)
```

Findings :

- The mortality rate of all three age groups are generally decreasing as a whole
- The mortality rate of the adult group, however, have fluctuation within the period


```{r message=FALSE, warning=FALSE, include=FALSE}
head(le_dropped)
```

```{r include=FALSE, warning=FALSE, include=FALSE}

#df = subset(le_dropped, select = -c(Country,GDP,Population) )
df = subset(le_dropped, select = -c(Country) )
#head(df)
```


For rest of the analysis, we drop the country column so that we can understand general trends in life expectancy
independent of countries. Let's visualize the distribution of response variable using below histogram.
```{r message=FALSE, warning=FALSE,echo=FALSE,fig.height=3,fig.width=3}
library(Hmisc)
#library(GGally)
#ggpairs(df)
df_hist <- df
colnames(df_hist)[3]<-"LifeExp"
hist.data.frame(df_hist[3])
```

Findings :
- As we see, the response variable Life Expectancy is normally distributed and so our first try is to see if MLR is able to predict well. 
- Also, using this model and running a BIC on it, we can understand the columns that are important.

## Predictor Space

We now turn our focus to look at the different predictor variables. Following shows the correlation of different variables and their spread in the dataset.


```{r message=FALSE, warning=FALSE, echo=FALSE}

#install.packages(GGally)
library(GGally)
ggcorr(df,palette = "RdBu", size=2,label=TRUE,label_size = 2,hjust = .95,layout.exp=2)

```

Summary of correlation for >0.7 or <-0.7: infant.deaths and under.five.deaths is nearly 100% correlated with each other and Polio is highly correlated with Diphtheria, Income composition of resources is highly correlated with Schooling.Both thinness variables are highly correlated. Other variables are low to moderately correlated.
The response variable Life expectancy is highly correlated with Income composition, Schooling and adult mortality variables.

### Initial Modelling and Variable Importance:

As response variable Life.Expectancy is approximately normally distributed, first step is to try lm model for this data and also run BIC to get the variable selection from the dataset.

The dataset has 20+ predictors and based on correlation plot there are correlation between the variables, BIC would help to eliminate some of the predictors that are conveying same signal as others and also explains less variability in life expectancy.

```{r message=FALSE, warning=FALSE}
lmmod <- lm(Life.expectancy~., data = df)
summary(lmmod)
```

```{r echo=FALSE,message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(lmmod)
```

Residuals related plots: The residuals versus fits plot would provide us with information on the residual against the fitted values in regression analysis. This could be used to identify the patterns in the residuals that may indicate the model is not capturing the relationship between our predictor and the outcome variable, therefore, allowing us to detect any non-linearity, unequal error variances and outliers.  In general, we would want to see our residual randomly scattered around 0 since this indicates that the model assumption is met and is a good fit for the data. However, from the above-plotted residuals versus fits plot, we could see there is a curvature shape to our residuals and there is a presence of outliers and high leverage points on the left-hand side of the residuals versus fits the plot, This could be problematic since outliers and leverage points could have a significant impact on the regression coefficient. And the curved shape indicates that our model may be misspecified and further investigation is needed. Also, same is indicated by the scale-location plat with standardized residuals > 1.5. Based on leverage plot, we do not see the need to remove any outliers in this initial assessment. 

QQ plot: QQ-plot (Quantile-quantile plot) allow us to investigate the univariate normality of the dataset. If the points on the QQ-plot fall approximately along a straight line, it suggests that the sample comes from a population with similar distribution to the theoretical distribution that we are comparing to. From the QQ-plot that we have plotted above, the point deviates from a straight line on both ends and indicates there is a heavy tail. 

We make a note of the structures in the initial assessment and acknowledge they could impact the MLR performance. During next stage of the project, we plan to address the issues identified in diagnostics during final modeling.

Variable selection methods: Following is the summary from a couple of common methods used in variable section of models: BIC(Bayesian information criterion, backward selection) and VIF (Variable Inflation Factor).

```{r message=FALSE, warning=FALSE, include=FALSE}
model.step.bic <- step(lmmod,k=log(nrow(df)))
summary(model.step.bic)
```

Initially, all 20 variables were used in our model and achieved an AIC score of 7642.14. After performing BIC backward step model selection method, The BIC backward step model selection method has reduced our model’s independent variable to 13 and achieved a lower AIC score of 7604.34. Since a lower AIC score signifies the regression is a better fit to the data, meaning that after removing some irrelevant variable in our data set, the simple model is still able to explain the data well well and has improved the fitting from the initial model. Also the final reduced model has an Adjusted R-squared score of 0.8296 compared to the original model’s 0.8299 isn’t much of a drop in the Adjusted R-squared score meaning our BIC reduced model was still able to have the same amount of variability.

Now going back to variable selection, this time we will be using VIF(Variance Inflation Factor) to investigate whether it is possible to come up with a better model from BIC reduced model by eliminating some highly correlated variables in the data. 

```{r message=FALSE, warning=FALSE, include=FALSE}
library(car)
vif(model.step.bic)

#removed one of infant.deaths or under.five.deaths
```
VIF(Variance Inflation Factor) is a variable selection method that is used to identify and eliminate highly correlated variables in a regression model. If the VIF value for a variable is high, it indicates that the variable is highly correlated with another predictor within the model. From the above output, we could see that variables “infant.deaths” and “under.five.deaths” are highly correlated. Therefore we will have to remove either “infant.deaths” or “under.five.deaths” to resolve the multicollinearity within our data inorder to improve our model accuracy and interpretability. We acknowledge this may be necessary for next level of tuning the model.
```{r message=FALSE, warning=FALSE, include=FALSE}
df1<-df[,c('Life.expectancy','Status','Adult.Mortality','infant.deaths','under.five.deaths',
      'Hepatitis.B','BMI','Polio','Diphtheria',
      'HIV.AIDS','thinness..1.19.years','Income.composition.of.resources','Schooling','GDP')]

df1$Status <- factor(df1$Status)
#removed 'under.five.deaths', due to vif - multi collinearity from BIC selection but it decreased model
#performance so should we retain it?
#head(df1)
```

After doing variable reduction and selecting only variables that are recommended by BIC (Status + Adult.Mortality + infant.deaths + Hepatitis.B + BMI + under.five.deaths + Polio + Diphtheria + HIV.AIDS + GDP + thinness..1.19.years + Income.composition.of.resources + Schooling), we move on checking for any clustering effects in data. 

```{r message=FALSE, warning=FALSE}
library(mclust)
clus1 <- Mclust(df1)
summary(clus1)
```
Looks like there are some clusters in the data, first understanding is it is because of variable “Status”’s developed vs developing. Acknowledging this information which may be helpful in future phases of model building and fine tuning. For example, if MLR would be the final model, building interaction with cluster variable and rest of data would further improve model performance.

Using our reduced model, we feed that data to Linear Model and achieve an Adjusted R-squared of 0.8296 and all variables have a p-value of less the 0.05, meaning that all of our independent variables within the model is statically significant to our dependent variable, In other words, there is strong evidence against the null hypothesis, suggesting that the observed relation between other dependent variable and independent variable is significant and real, not just due to random variation or chance.

```{r message=FALSE, warning=FALSE, include=FALSE}
lmmod2 <- lm(Life.expectancy~.,data=df1)
summary(lmmod2)
vif(lmmod2)
```

### Conclusion/Key Findings

Summarized below are some key findings from EDA.

- Response variable is looking to be normally distributed and initial model score is ~82% which means this model is able to explain 82% of variation in Life expectancy. Its possible to use multiple linear regression for this data. From the diagnostic plots it may be seen that there is skewness in the data.
- There are some variables that suffer multi collinearity (from VIF) scores.
- Not all predictors are necessary to describe response variable. Model selection will be helpful.

Due to the spread of data (clustering, non-linearity of predictors w response variable, skewness in data), it is necessary to explore other models specifically non-parametric regression models.


## Questions and Next Steps

1. Does the variables selected using BIC and linear model able to explain Life Expectancy adequately? A hypotheses test is required for this.

2. Is the response variable normally distributed? Shapiro-Wilk test for normality will need to be conducted for this.

3. Is Multi linear regression the best model or go with other non parametric models? From initial feedback, there are hypotheses tests available to validate this.

Need to explore further on these questions from proposal stage and conclude.

4. Understanding impact of individually controlled factors - The dataset has all predicting variables divided into 4 groups: Immunization related factors, Mortality factors, Economical factors and Social factors. Some of these factors are controllable by individuals like immunization, alcohol etc. Some of these factors are noncontrollable and macro elements like GDP. If an individual within a country want to improve life expectancy, how much is controllable/can be influenced personally? What proportion of variation in life expectancy can be explained by these variables? For example, What is the effect of “Alcohol/BMI” on the life expectancy?

5. Understanding impact of Government/Public controlled factors - From Government perspective, how are the preventive measures influencing life expectancy? What proportion of variation in life expectancy can be explained by these variables? For example, Does Higher health expenditure (column H) on Health improve life expectancy?



```{r message=FALSE, warning=FALSE, include=FALSE}
## Extra - add if there is page available

#str(le)
class(df$Country)
glm_model <- glm(Life.expectancy~., data = df, family = "gaussian")
bic_back <- step(glm_model, k=log(nrow(df)), direction="backward", trace=FALSE)
summary(bic_back)
summary(glm_model)
```



```{r message=FALSE, warning=FALSE, include=FALSE}
###############################################################################
###############PCA(suggest a simpler scoring system):##########################
###############################################################################
#head(df[,-3])
head(df[,c(-1,-3)])
df$Status <- as.numeric(as.factor(df$Status)) #Convert Non-Numeric Columns to Numeric for PCA
pca_model <- prcomp(df[,c(-1,-3)],scale.=TRUE)
summary(pca_model)

```
```{r message=FALSE, warning=FALSE, include=FALSE}
#Use Sceen Plot to decide how many PC to keep, But there seems to be a large value in our PC1 Standard Deviation so it isn't a clear indicator.

plot(pca_model, type="lines")
```
```{r message=FALSE, warning=FALSE, include=FALSE}
#Use Standard deviation instead to decide how many PC to keep, if it is below 1 we toss it out since it does not explain much info about our data.it is suggest us to keep 15 PC.

pca_model$rotation[,1:15]

###############################################################################
#########################################FA:(not working)###################################
###############################################################################


df2 = subset(le_dropped, select = -c(Country) )
head(df2)

#Need to eliminate non numeric variables before running factor analysis below
head(df)
#fa_model <- factanal(df2, factors = 13)
```


